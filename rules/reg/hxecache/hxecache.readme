README for hxecache exerciser
================================================================================
Last update:

Table of contents:
================================================================================
1)  Goal
2)  Salient Features
3)  Hardware supported & HW test scope
4)  Supported OS
5)  General info
6)  Prerequisites
7)  Brief Description
8)  Sample Rule Stanza and Explanation
9)  Exerciser specific Log files
10) Understanding htxstats
11) Common config errors
12) Miscompare analysis
13) Limitations


================================================================================

1) Goal:
--------
The goal of hxecache testcase is stress system cpu d-caches & related hardware mechanisms,
by means of the following:
1. A targeted cache rollover (L2 and/or L3) 
2. Disrupting/irritating data streams of hardware prefetch engines.

2) Salient Features:
--------------------
1. Targetted L2/L3 rollover supported .
2. Prefetch irritator testcase supported. ( To run as a standalone prefetch irritator, to disrupt
   hardware prefetch streams generated by other exercisers )
3. Upto 4 separate algorithms supported for prefetch irritator.
4. Individual control of each of the prefetch algorithms supported.
5. Exclusive memory used for prefetch threads.
6. Supports an option to have control on number of cpus to be enabled for exercsising.

3)Hardware supported & HW test scope:
-------------------------------------
1. Targetted cache Rollover supported for Power7 and later only.
2. Creation of cache bounces supported till Power8 only.
3. Prefetch irritator algorithm supported for Power6 and later.
4. Stride-n, Partial & Transient prefetch support for Power7 and later only.

4) Supported OS:
----------------
AIX: y
Linux: y
BML: y

5) General info:
----------------
HTX hxecache runs as a user application on top of the OS (AIX,BML,Linux).
In case of miscompare, the exerciser traps to kdb. 

Quickstart:
hxecache is a part of mdt.bu (AIX) and mdt.all(BML,Linux). Therefore a quickstart of 
htx with the default mdt file will start the hxecache exerciser too.

6) Prerequisites:
-----------------
- Min h/w : Power6 and later

- s/w config :  Requisite amount of hugepages must be available.
      	        In a normal htx enviornment, the htx setup scripts do the
		job of reserving these pages.

7) Description:
-----------------

Theory of Cache Access Pattern for Maximum Traffic In/Out -

Size of testcase memory to use is detected by the size of the cache you are targeting.
To ensure that you are able to fill and to roll, or cause every line to be replaced in
the cache, you must use a contiguous block that is a minimum of twice the size of the cache.  

The order in which this memory is accessed is controlled by a minimum of 2 nested loops.  The outer
loop increments the address by the size of a cache line.  The inner loop increments the address
by the size of a set.  Set size is determined by dividing the cache size in bytes by the associativity.
Performing accesses in this manner means that at most, n accesses will be performed before a line
has to be replaced in the cache, where n is equal to the associativity of the cache (16 for the case of
P6 L3). If sequential accesses are used, y accesses may be performed at most before lines are replaced,
where y is equal to the cache size divided by the cache line size (256K for the case of P6 L3).  

set_size = cache_size / cache_asc;
number_lines_in_set = set_size / line size;

addr = testcase memory start;

for (i=0;i<number_lines_in_set;i++,addr+=line_size) {
       for (j=0;j<(cache_asc*2);j++, addr+=set_zize) {
              // perform access at addr (could be a load or store to a ptr=addr)
       }
}


Following is a diagram representation of sets view of physically contiguous memory set:


	--------> Inner For Loop jumps by cache set size
	|
	|
	|
	V
	 Outer For Loop jumps by cache line size


								  Memory Set
			(asc - 1)th set
			----------------			----------------  0 byte
			|		|<---- Set		|		 |
		1	|		|			|16 M page	 |
		---------------	|			|		 |
		|		|	|			|----------------|
0th set	|		|				|		 |
         ---------------	|			|16 M page  |
0 byte 	cache line	|				|		 |		 
	   ---------------	<------	  	|----------------|
			|		Cache View		|		 |
			|					|16 M page	 |
			|					|		 |
			|					|----------------|
			|					|		 |
			|					|16 M page	 |
			|					|		 |
							 	----------------  2*cache_size - 1
	----------------	
(set_size -1)	

Explanation of Testcases supported -

a. CACHE_BOUNCE :

Note: On P9 and above architectures, Bounce test case is disabled from hxecache exerciser.

The example above allows for the memory to be accessed in one particular order - increasing from
the start of the testcase memory towards the end, where every testcase will touch the same sets,
classes, and bytes in exactly the same order every time. It allows to target specific set also.

This is accomplished by making the following types of modifications:

1. changing the access of the address from using pointers in a precompiled way to calling a different function  
   depending on how the different types of loads and stores are supported
   (lbz, lhz, lwz, ld). 
2. On SMP box, program creates as many threads (pthreads) as mentioned in rulefile and all of them do
   write/read/compare operations.
3. Every thread operates onto its own strip in 128 byte cache line as mentioned below.

	----------------------------------------------------
	thread 0 | thread 1 | 		-----	   thread n |	
	----------------------------------------------------
	
	This is how 8 byte pattern is used for various lengths of load/stores :
	
	0x0102030405060708
	  ** For byte load/store
	  **** For short load/store
	  ******** For int load/store
	  **************** For long load/store

This access pattern, where different threads bound to different cpus access the memory that maps one physical 
instance of cache, causes bouncing of the cache line to occur across the various physical caches of the cpus.

b. CACHE_ROLLOVER :

Another testcase provided is the cache rollover testcase.There are 2 cases here.

With the prefetch irritator turned on, The access patterns of the threads on the memory that maps the caches 
is slightly different:


			:										:
			:										:			
			:										:
	----------------------------------------------------                        ----------------------------------------------------
	thread 0........				   |			    thread 1........				       |
	----------------------------------------------------                        ----------------------------------------------------
			:										:
			:										:
			:										:
			V										V

The memory segment allocated is divided into several parts, each mapping a physical instance of cache.
there are 2 threads per core are made to operate on their own memory equal to physical instance of cache size.
This way both threads would result in to roll over of the cache for their own  core.
This achieves a rollover of all the caches. 
(In P8 and below, the same will be acheived by 1 thread per core and using twice of physical instance of cache size)

With prefetch on, in each sub group, there is only one thread(2 threads in P9) to operate on the cache and the rest (depending on smt value) are the 
prefetch threads. Therefore the access patterns are as shown above.

When the prefetch irritator is turned off, and each core is configured at smt > 1( smt>2 for P9), there is no change in number of threads for cache roll operation.



SYNC SUPPORT (May work only for P8 and below architectures)
-----------------
For both the above test cases, a synchronization mechanism among threads is implemented to make it more effective.
We make use of three sync points, used in the beginning, middle and end of W/R/C operation. This technique ensures 
that all the threads sync up in the beginning to start executing W/R/C operation simultaneously. After completion 
of write operation, all the threads re-sync again before proceeding to R/C operation. Once all the threads are done 
reading and comparing, they move on to next iteration simultaneously.

The sync mechanism is optional and can be turned off from the rule file.

SYNC MECHANISM
--------------

pthread_cond_wait and pthread_cond_broadcast APIs from pthread library is used to create synchronisation between threads.
The mechanism used is as follows. 
A separate function called synchronize_threads is called. In the function a static counter ( for the rule stanza ) is kept 
to keep track of the count of the threads which have called this function. The counter is incremented everytime the function is called.
The count value (after incrementing) is mod-d by the number of cache threads running.

count = count % num_cache_threads;

In case the result is not 0 ( or the exit_flag is not set ), then the thread performs a wait on a condition variable.
In case the result is 0 ( or external exit flag has been set because of shutdown initiation), then the threads (if any) waiting 
on the condition variable are broadcasted to proceed, and they are not blocked any more.

In this way, as long as the counter does not count to a value equal to number of current running cache threads, all threads wait.
The moment all threads have called this function (essentially counting upto number of cache threads running), all threads are awoken
and start executing in unison.

c. PREFETCH_ONLY :

This is another testcase that is now offered. The intention is to have hxecache run stand alone with only the 
prefetch irritator component enabled so that it functions to disrupt the prefetch streams initiated by other 
exercisers. This testcase is only valid when the prefetch irritator is enabled.

The Prefetch Irritator -

Prefetch Irritator is a component of hxecache exerciser and can be turned off/on from rule file. Main goal of prefetch 
irritator function of hxecache is to kick off software initiated prefetch engines in a random fashion on a different memory 
set which is used by hxecache.

Prefetch engines are initialised to kick off prefetching alongside cachestress program. Various versions of dcbt and dcbtst 
are used to perform the above by randomizing all the possible fields.

Prefetch irritator threads work on their exclusive memory and do not share memory with the hxecache threads.This allows for 
more effective disruption of prefetch streams initiated by hxecache threads OR by threads of other exercisers.

The number of prefetch threads created depends on the number of logical cpus present on the system. 

Normally for ST 16 streams can be allocated but in SMT case  each thread can have a maximum of 8 streams [8*2 maximum number].
If more streams need to allocated then the existing streams are replaced. 

Prefetch Irritator thread obtains the starting address and the specific prefetch algorithm to run. Each prefetch thread acts 
on its own exclusive memory area. To allow for a conservative memory model, we have 2 prefetch threads sharing one 16M page.
The number of pages on which prefetch irritator will operate on is calculated assuming 16M pages are being used.

Each of these threads would first initialise the prefetch streams using various forms of dcbt and dcbtst to kick off prefetch engine.
 
All the inputs that are fed to preftch irritator are seed based and that includes :

a) Offset address  of the page from where the irritator starts prefetching.
b) Offset within cache line  that is consumed to keep prefetch running.
c) Direction of prefetching i.e either forward or backward.
d) Depth of prefecting.

For every load, L1 data are prefetched and every store,  L2 data is prefetched. Two lines and a maximum of 24 lines ahead 
can be prefetched for L1 and L2 respectively . 
 
In order to keep the prefetch engine running data has to be consumed in sequential manner. Prefetch threads run in parallel 
with cache stressing threads of hxecache. As irritator would keep fetching data at random addresses this it would try disrupting 
the execution of cache stress which is basic idea of the prefetch irritator.

Variants of Prefetch irritator for P7 ---

1. Prefetch N stride :   This is a stream variant of dcbt instruction which  specifies which data units 
   to access  that are not adjacent in an address space but are separated by a fixed distance i.e. stride manner.
   This data stream variant specifies the following which are randomized using a seed for various runs  : 
·	Word Granularity access stride 
·	Unit size 
   Streams are consumed by executing load instructions from the EA prefetched but in strided manner. 

2. Transient dcbt : This is an enhancement to the dcbt instruction such that with 
   TH=0b10000, it provides a hint that usage of a soon-to-be-referenced datum is 
   transient. This hint may be used to trigger a prefetch of data into the processor's cache 
   hierarchy in a way that minimizes displacement of other data whose usage has not been
   identified as transient Using the mentioned basic structure of prefetch , 
   engines are kicked off  but the only difference being the data would be transient  .
   Streams are consumed by executing load instructions from the EA prefetched , one word /cache line.    

3. Partial dcbt : This is an enhancement to the dcbt instruction such that with TH=0b11000 it provides a hint that 
   a soon-to-be-referenced datum has limited spatial locality. This hint might be used to trigger a  prefetch of data
   into the processor's cache hierarchy using a partial cache block transaction on the memory bus, thus reducing 
   memory bus bandwidth usage . In this test case one chooses the size in a random fashion and kicks off
   prefetch engines for a given EA. Streams are consumed by executing load instructions from the same EA
   prefetched , one word /(2+24+1)cache lines , as data shouldn't be in L3 prior . 

Each of these variants OR any combination of the above can be enabled/disabled  via the rule file.

* Note:
Only the default prefetch irritator is supported for P6. The above mentioned specialised algorithms are only
valid if set on P7 and later.
There is no separate global flag to turn off the prefetch irritator. A combination of "OFF" for all prefetch algorithms
constitutes the prefetch irritator being disabled.

SETUP
-----
							
Each instance of hxecache , By default (cpu_filter=N*P*C*T*) spawns 2 hxecache threads per which runs on 
0th and SMT/2 logical processors respectively for cache roll over opeartion. On remaining threads prefetch operation is performed. 
Prefetch Irritator testcase wouldn't be executed when it is SMT2 mode or if cpu_filter results in 2 threads per core, irrespective of rule file prefetch value.


Each instance of hxecache , spawns 2:2 hxecache to prefetch threads which run on each logical cpu with SMT=4 is ON .
SMT = 4 ON , cache threads would run on logical cpu 0 and 2. Each variant of prefetch irritator would run on other two
logical cpus . N_stride , partial and transient are the three variants of prefetch irritator.  


8) Sample Rule Stanza for P9 with explanation:
---------------------------------------

rule_id			= L3Roll_Prefetch_1
target_cache	 	= L3
target_set 		= -1
width			= 8
crash_on_misc		= yes
num_oper		= 100
prefetch_irritator	= 1
prefetch_nstride	= 1
prefetch_partial	= 1
prefetch_transient	= 0
cache_testcase_type   	= CACHE_ROLL_WITH_PREF
cpu_filter		= N*P*C*T*


Explanations of rule file keywords:

rule_id 	- Indicates rule stanza number. We support upto 10 rule stanzas in the rule files for hxecache.
		  The rule string has to end with an underscore and a number. (e.g. rule_1)
target_cache	- Indicates the cache under test i.e L2 cache or L3 cache.
compare		- (P8 and below)Indicates if a compare should be done after a write to cache memory , 1 => YES 0 => NO
target_set  	- Refers to any specific target set to stress. 16 way L3 cache will take values from 0-15.
	      	  Default is -1, where all sets are targeted.

data_width    	- (P8 and below)Width of data to be written to the cache memory. 
	          i.e 0 => byte, 1 => short, 2 => int, 3 => long int, 4 => random 
crash_on_misc   - (P8 and below)Whether to crash to kernel debugger incase of miscompare.
		  0 => NO 1 => YES
    		  0 - No, 1 - Yes. 

num_oper 	- Number of times L2 or L3 cache would roll over.
	  	  0 => Infinitely  
	  	  Suitable values for P6 are < 50 to avoid unusually long execution times.

Each prefetch algorithm can be enabled/disabled using the below where 0 => OFF 1=> ON.
prefetch_irritator	0
prefetch_nstride	1
prefetch_partial	1
prefetch_transient	1


seed 		- (P8 and below)To run the test with a particular seed . Used for recreate in case of a failure. 0 - Normal run.
cache_testcase_type  	- Indicates which testcase is to run. 
	       	  CACHE_BOUNCE_WITH_PREF 	- This causes cache bouncing along with Prefetch threads. 1 cache thread and 3 prefetch threads per core.
		  CACHE_BOUNCE_ONLY	   	- Cache Bounce with 1 cache thread per core.		        	
		  CACHE_ROLL_WITH_PREFETCH	- This causes cache to rollover. 1 cache thread and 3 prefetch threads per core.
		  CACHE_ROLL_ONLY		- Cache Rollover with only cache threads and no Prefetch. SMT cache threads per core.
		  PREFETCH_ONLY			- Software Prefetching Testcase.

gang_size 	- (P8 and below)Indicates the number of physical instances of cache that will be covered.
	    	  Default 8.
	    	  Could also be set to 1 ONLY when running under the Equaliser framework.
	    	  
thread_sync	- (P8 and below)It is used to configure whether synchronisation among threads is desired or not.
		  yes - means Sync is desired
		  no  - means sync is not desired
		  By default sync is disabled.
		  
exclude_cores	- (P8 and below)To disable any particular core, mention the core number in this field.
 		  If multiple cores are to be excluded, separate them with comma(s).
 		  This field is optional and may not be included in the rule file if disabling cores is not desired.
		  for P9 and above architectures, this can be acheived using cpu_filter. 

For P9 and above, hxecahe exerciser is derived from htx nest framework, Rule parameters are same as that of hxemem64(memory exerciser).
please look into the read me of hxemem64 for detailed explaination of rules. 

#ls $HTXREGRULES/hxemem64/hxemem64.readme
/usr/lpp/htx/rules/reg//hxemem64/hxemem64.readme



9) Exerciser specific Log files:
--------------------------------
1.(P8 and below): 
$HTX_LOG_DIR/htx/hxecache/cache<x>/hxecache<x>.runlog (where x is the instance number) 
	- contains run time log from exerciser. By default logging is very minimum and dumps only rulefile paramters.
2.(P9 and above):
$HTX_LOG_DIR/htx/hxecache/cache<x>/cache<x>_log  (where x is the instance number)
	- Contains detailed thread information.

2. /tmp/hxecache_miscompare.ab.cde.xxxxxx - It is generated ONLY in case of a miscompare. It contains the hexdump of the 16M page where a thread has found miscompare.


10) Understanding htxstats:
---------------------------

Hxecache statistics appear as follows and are primarily a figure of bytes read/written per 
second (and per instance ofcourse):

cache0:
  cycles                              =                  0
  # good reads                        =               4000
  # bytes read                        =         2097152000
  # good writes                       =               4000
  # total instructions                =                  0
  # bytes written                     =         2097152000
  # good others                       =                  0
  # bad others                        =                  0
  # bad reads                         =                  0
  # bad writes                        =                  0
  # data transfer rate(bytes_wrtn/s)  =         4744688.00
  # data transfer rate(bytes_read/s)  =         4744688.00
  # instruction throughput(MIPS)      =           0.000000



11) Common config errors:
-------------------------

If cache_testcase_type is set to CACHE_TEST_OFF without any of the prefetch algorithms being enabled, the
testcase would be ineffective.

If num_oper is set too high for any exerciser. ( This number varies across systems and configurations ), the
execution time may be unusually high, and may report the exerciser as HUNG.

The optimal range for P6 would be between 10 and 50.
The optimal range for P7 and above  would be between 100 and 1000.


12) Miscompare analysis:
------------------------

ERROR MESSAGES 
--------------
*************
In case of miscompare, following details are dumped in HTX error log and message log:

Exact prints from /tmp/htxerr should be like following :

/dev/cache        Feb  6 05:22:20 2007 err=00000000 sev=1 hxecache

 RB: TID: 0 miscompare !! EA = 0x5000002b data = 0 pattern = ff

KDB registers will contain :
r3 - 0xBEEFDEAD
r4 - EA of miscompare location
r5 - pointer to 8 byte pattern
r6 - address of memory_set structure
r7 - address of system_information structure
r8 - Thread no [ sequence of creation ] 
r9 - Pointer to thread_context structure for the thread.
r10 - Pointer to current index of rule_info structure.

for P9 and above:
r3 - 0xBEEFDEAD
r4 - EA of miscompare location
r5 - pointer to 8 byte pattern
r6 - thread number.
r7 - thread structure pointer.
r8 - cache_exer_info global pointer.
r9 - g_data pointer.
r10 - current stanza pointer

For debugging hxecache miscompare on KDB prompt, compare data pointed to by r4 and r5.
Amount of data that needs to be compared is in thrad_context structure.

After debug is done, please continue (and do not reboot) from KDB prompt. It will generate a miscompare file, which will have hexdump of the 16M 
page where the thread detected a miscompare.
The name of the miscompare file will be /tmp/hxecache_miscompare.ab.cde.xxxxxx
where,
ab     = instance number of cache exerciser
cde    = index of the miscomparing thread
xxxxxx = seed for the thread.

*************
If you see following error in /tmp/htxerr, it indicates that hxecache could not find
required amount of physically contiguous memory and hence it could not run. In such
scenario, please reboot the system and try once again.

/dev/cache        Mar 24 11:55:39 2007 err=ffffffff sev=1 hxecache

 Could not get physically contig mem....Exiting....
*************

13) Limitations:
----------------
Current limitations:

a. Hxecache does not support Halting and restarting the run of the exerciser [which is done through option 2 from supervisor 

b. Cannot perform a cache rollover for Power 6, due the the huge memory requirements.
   No plans to support the same in the future.
   
c. This exerciser is not supported on shared LPARs. It will only run (/dev/cachex devices will be created) only on a dedicated LPAR.
